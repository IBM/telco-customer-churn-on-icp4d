{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telco Customer Churn for ICP4D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this notebook to create a machine learning model to predict customer churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user watson-machine-learning-client --upgrade | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Load and Clean data\n",
    "We'll load our data as a pandas data frame.\n",
    "\n",
    "* Highlight the cell below by clicking it.\n",
    "* Click the `10/01` \"Find data\" icon in the upper right of the notebook.\n",
    "* If you are using the [ICP4D Learning Path] and have the [Virtualized data], begin by choosing the `Remote` tab. Then choose your virtualized data (i.e. User<xyz>.billingProductCustomers), click `Insert to code` and choose `Insert Pandas DataFrame`.\n",
    "* If you are using this code pattern outside of the [ICP4D Learning Path], add the locally uploaded file `Telco-Customer-Churn.csv` by choosing the 'Local` tab. Then choose the `Telco-Customer-Churn.csv`, click `Insert to code` and choose `Insert Pandas DataFrame`.\n",
    "* The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below.\n",
    "* Run the cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place cursor below and insert the Pandas DataFrame for the Telco churn data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Pandas naming convention `df` for our DataFrame.  Make sure that the cell below uses the name for the dataframe used above, i.e df1, df2,... dfX for the remote data using Data Virtualization, or df_data1, df_data2, ... if using local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df1\n",
    "df = df_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Drop CustomerID feature (column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('customerID', axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Examine the data types of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Convert TotalCharges column to numeric as it is detected as object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = pd.to_numeric(df.iloc[:, 18], errors='coerce')\n",
    "new_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Modify our dataframe to reflect the new datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 18] = pd.Series(new_col)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.5 Any NaN values should be removed to create a more accurate model. Prior examination shows NaN values for `TotalCharges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have any NaN values\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `nan_column` to the column number for TotalCharges (starting at 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_column = df.columns.get_loc(\"TotalCharges\")\n",
    "print(nan_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for nan_column (TotalCharges)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n",
    "\n",
    "df.iloc[:, nan_column] = imp.fit_transform(df.iloc[:, nan_column].values.reshape(-1, 1))\n",
    "df.iloc[:, nan_column] = pd.Series(df.iloc[:, nan_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have any NaN values\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, svm\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import PolynomialFeatures, LabelEncoder, StandardScaler\n",
    "import sklearn.feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Tenure Frequency count\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_palette(\"hls\", 3)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax = sns.countplot(x=\"tenure\", hue=\"Churn\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Tenure Frequency count\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_palette(\"hls\", 3)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax = sns.countplot(x=\"Contract\", hue=\"Churn\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Tenure Frequency count\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_palette(\"hls\", 3)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax = sns.countplot(x=\"TechSupport\", hue=\"Churn\", data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Grid for pairwise relationships\n",
    "gr = sns.PairGrid(df, size=5, hue=\"Churn\")\n",
    "gr = gr.map_diag(plt.hist)\n",
    "gr = gr.map_offdiag(plt.scatter)\n",
    "gr = gr.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalCharge  = df.columns.get_loc(\"TotalCharges\")\n",
    "print(nan_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plot size\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "# Attributes destribution\n",
    "a = sns.boxplot(orient=\"v\", palette=\"hls\", data=df.iloc[:, totalCharge], fliersize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Charges data distribution\n",
    "histogram = sns.distplot(df.iloc[:, totalCharge], hist=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure  = df.columns.get_loc(\"tenure\")\n",
    "print(tenure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenure data distribution\n",
    "histogram = sns.distplot(df.iloc[:, tenure], hist=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "monthly = df.columns.get_loc(\"MonthlyCharges\")\n",
    "print(monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly Charges data distribution\n",
    "histogram = sns.distplot(df.iloc[:, monthly], hist=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Understand Data DistributionÂ¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_data = spark.createDataFrame(df)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = df_data\n",
    "(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\n",
    "\n",
    "print(\"Number of records for training: \" + str(train_data.count()))\n",
    "print(\"Number of records for evaluation: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Examine the Spark DataFrame Schema\n",
    "Look at the data types to determine requirements for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Use StringIndexer to encode a string column of labels to a column of label indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model\n",
    "\n",
    "\n",
    "si_gender = StringIndexer(inputCol = 'gender', outputCol = 'gender_IX')\n",
    "si_Partner = StringIndexer(inputCol = 'Partner', outputCol = 'Partner_IX')\n",
    "si_Dependents = StringIndexer(inputCol = 'Dependents', outputCol = 'Dependents_IX')\n",
    "si_PhoneService = StringIndexer(inputCol = 'PhoneService', outputCol = 'PhoneService_IX')\n",
    "si_MultipleLines = StringIndexer(inputCol = 'MultipleLines', outputCol = 'MultipleLines_IX')\n",
    "si_InternetService = StringIndexer(inputCol = 'InternetService', outputCol = 'InternetService_IX')\n",
    "si_OnlineSecurity = StringIndexer(inputCol = 'OnlineSecurity', outputCol = 'OnlineSecurity_IX')\n",
    "si_OnlineBackup = StringIndexer(inputCol = 'OnlineBackup', outputCol = 'OnlineBackup_IX')\n",
    "si_DeviceProtection = StringIndexer(inputCol = 'DeviceProtection', outputCol = 'DeviceProtection_IX')\n",
    "si_TechSupport = StringIndexer(inputCol = 'TechSupport', outputCol = 'TechSupport_IX')\n",
    "si_StreamingTV = StringIndexer(inputCol = 'StreamingTV', outputCol = 'StreamingTV_IX')\n",
    "si_StreamingMovies = StringIndexer(inputCol = 'StreamingMovies', outputCol = 'StreamingMovies_IX')\n",
    "si_Contract = StringIndexer(inputCol = 'Contract', outputCol = 'Contract_IX')\n",
    "si_PaperlessBilling = StringIndexer(inputCol = 'PaperlessBilling', outputCol = 'PaperlessBilling_IX')\n",
    "si_PaymentMethod = StringIndexer(inputCol = 'PaymentMethod', outputCol = 'PaymentMethod_IX')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_Label = StringIndexer(inputCol=\"Churn\", outputCol=\"label\").fit(spark_df)\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_Label.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_features = VectorAssembler(inputCols=['gender_IX',  'SeniorCitizen', 'Partner_IX', 'Dependents_IX', 'PhoneService_IX', 'MultipleLines_IX', 'InternetService_IX', \\\n",
    "                                         'OnlineSecurity_IX', 'OnlineBackup_IX', 'DeviceProtection_IX', 'TechSupport_IX', 'StreamingTV_IX', 'StreamingMovies_IX', \\\n",
    "                                         'Contract_IX', 'PaperlessBilling_IX', 'PaymentMethod_IX', 'TotalCharges', 'MonthlyCharges'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Create a pipeline, and fit a model using RandomForestClassifier \n",
    "Assemble all the stages into a pipeline. We don't expect a clean linear regression, so we'll use RandomForestClassifier to find the best decision tree for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[si_gender, si_Partner, si_Dependents, si_PhoneService, si_MultipleLines, si_InternetService, si_OnlineSecurity, si_OnlineBackup, si_DeviceProtection, \\\n",
    "                            si_TechSupport, si_StreamingTV, si_StreamingMovies, si_Contract, si_PaperlessBilling, si_PaymentMethod, si_Label, va_features, \\\n",
    "                            classifier, label_converter])\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "evaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "area_under_curve = evaluatorDT.evaluate(predictions)\n",
    "\n",
    "#default evaluation is areaUnderROC\n",
    "print(\"areaUnderROC = %g\" % area_under_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Save the model and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a unique name for MODEL_NAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"myname model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Save the model to ICP4D local Watson Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsx_ml.ml import save\n",
    "\n",
    "save(name=MODEL_NAME, model=model, test_data=test_data, algorithm_type='Classification',\n",
    "     description='This is a SparkML Model to Classify Telco Customer Churn Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Write the test data without label to a .csv so that we can later use it for batch scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_score_CSV=test_data.toPandas().drop(['Churn'], axis=1)\n",
    "write_score_CSV.to_csv('../datasets/TelcoCustomerSparkMLBatchScore.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Write the test data to a .csv so that we can later use it for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_eval_CSV=test_data.toPandas()\n",
    "write_eval_CSV.to_csv('../datasets/TelcoCustomerSparkMLEval.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you have created a model based on customer churn data, and deployed it to Watson Machine Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
